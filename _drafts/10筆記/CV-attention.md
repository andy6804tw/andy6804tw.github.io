
## 模型結構簡介
為了更清楚地介紹計算機視覺中的注意力機制，這篇文章將從注意力域（attention domain）的角度來分析幾種注意力的實現方法。其中主要是三種注意力域，空間域(spatial domain)，通道域(channel domain)，混合域(mixed domain)。

（1） 空間域

設計思路：

Spatial Transformer Networks（STN）模型是15年NIPS上的文章，這篇文章通過注意力機制，將原始圖片中的空間信息變換到另一個空間中並保留了關鍵信息。這篇文章的思想非常巧妙，因為卷積神經網絡中的池化層（pooling layer）直接用一些max pooling 或者average pooling 的方法，將圖片信息壓縮，減少運算量提升準確率。但是這篇文章認為之前pooling的方法太過於暴力，直接將信息合併會導致關鍵信息無法識別出來，所以提出了一個叫空間轉換器（spatial transformer）的模塊，將圖片中的的空間域信息做對應的空間變換，從而能將關鍵的信息提取出來。

（2） 通道域

設計思路：

通道域的注意力機制原理很簡單，我們可以從基本的信號變換的角度去理解。信號系統分析裡面，任何一個信號其實都可以寫成正弦波的線性組合，經過時頻變換之後，時域上連續的正弦波信號就可以用一個頻率信號數值代替了。

在卷積神經網絡中，每一張圖片初始會由（R，G，B）三通道表示出來，之後經過不同的捲積核之後，每一個通道又會生成新的信號，比如圖片特徵的每個通道使用64核卷積，就會產生64個新通道的矩陣（H,W,64），H,W分別表示圖片特徵的高度和寬度。每個通道的特徵其實就表示該圖片在不同卷積核上的分量，類似於時頻變換，而這裡面用卷積核的捲積類似於信號做了傅里葉變換，從而能夠將這個特徵一個通道的信息給分解成64個卷積核上的信號分量。既然每個信號都可以被分解成核函數上的分量，產生的新的64個通道對於關鍵信息的貢獻肯定有多有少，如果我們給每個通道上的信號都增加一個權重，來代表該通道與關鍵信息的相關度的話，這個權重越大，則表示相關度越高，也就是我們越需要去注意的通道了。

可參考SENet模型結構。

（3） 混合域

了解前兩種注意力域的設計思路後，簡單對比一下。首先，空間域的注意力是忽略了通道域中的信息，將每個通道中的圖片特徵同等處理，這種做法會將空間域變換方法局限在原始圖片特徵提取階段，應用在神經網絡層其他層的可解釋性不強。

而通道域的注意力是對一個通道內的信息直接全局平均池化，而忽略每一個通道內的局部信息，這種做法其實也是比較暴力的行為。所以結合兩種思路，就可以設計出混合域的注意力機制模型

