
人類可以自然且有效地在復雜的場景中察覺到突出的區域。受到這一點的啟發，注意機制被引入到電腦視覺中，其目的是模仿人類視覺系統。這種注意機制可以看作是一個基於輸入圖像特徵的動態權重調整過程。注意機制在圖像分類、目標檢測、語義分割、影片理解、圖像生成、3D 視覺、多模態任務和自監督學習等視覺任務中取得了巨大的成功。本篇文章參考這篇論文： [Attention Mechanisms in Computer Vision: A Survey](https://arxiv.org/abs/2111.07624.pdf) 總結了電腦視覺中的各種注意機制，並對所有 CV Attention 研究進行分類，例如：通道注意力、空間注意力、時間注意力和分支注意力。相關的論文研究整理也能參考 GitHub 上的 repo： [Awesome-Vision-Attentions
](https://github.com/MenghaoGuo/Awesome-Vision-Attentions) 原作者統整了目前電腦視覺領域中各種注意力機制的研究。

論文首先將基於注意力的模型在計算機視覺領域中的發展歷程大致歸為了四個階段：

1. 將循環神經網路與注意力機制相結合，代表性方法為 [RAM](https://www.cnblogs.com/wangxiaocvpr/p/5537454.html)
2. 明確預測判別性輸入特徵，代表性方法為 STN
3. 隱性且自適應地預測潛在的關鍵特徵，代表方法為 SENet
4. 自注意力機制



## 為什麼需要視覺注意力？
電腦視覺中的注意力機制的基本概念就是讓機器學會注意力，並能夠忽略無關的訊息從而關注重點訊息。至於為什麼要忽略無關的訊息呢？

## 模型結構簡介
為了更清楚地介紹計算機視覺中的注意力機制，這篇文章將從注意力域（attention domain）的角度來分析幾種注意力的實現方法。其中主要是三種注意力域，空間域(spatial domain)，通道域(channel domain)，混合域(mixed domain)。

（1） 空間域

設計思路：

Spatial Transformer Networks（STN）模型是15年NIPS上的文章，這篇文章通過注意力機制，將原始圖片中的空間信息變換到另一個空間中並保留了關鍵信息。這篇文章的思想非常巧妙，因為卷積神經網絡中的池化層（pooling layer）直接用一些max pooling 或者average pooling 的方法，將圖片信息壓縮，減少運算量提升準確率。但是這篇文章認為之前pooling的方法太過於暴力，直接將信息合併會導致關鍵信息無法識別出來，所以提出了一個叫空間轉換器（spatial transformer）的模塊，將圖片中的的空間域信息做對應的空間變換，從而能將關鍵的信息提取出來。

（2） 通道域

設計思路：

通道域的注意力機制原理很簡單，我們可以從基本的信號變換的角度去理解。信號系統分析裡面，任何一個信號其實都可以寫成正弦波的線性組合，經過時頻變換之後，時域上連續的正弦波信號就可以用一個頻率信號數值代替了。

在卷積神經網絡中，每一張圖片初始會由（R，G，B）三通道表示出來，之後經過不同的捲積核之後，每一個通道又會生成新的信號，比如圖片特徵的每個通道使用64核卷積，就會產生64個新通道的矩陣（H,W,64），H,W分別表示圖片特徵的高度和寬度。每個通道的特徵其實就表示該圖片在不同卷積核上的分量，類似於時頻變換，而這裡面用卷積核的捲積類似於信號做了傅里葉變換，從而能夠將這個特徵一個通道的信息給分解成64個卷積核上的信號分量。既然每個信號都可以被分解成核函數上的分量，產生的新的64個通道對於關鍵信息的貢獻肯定有多有少，如果我們給每個通道上的信號都增加一個權重，來代表該通道與關鍵信息的相關度的話，這個權重越大，則表示相關度越高，也就是我們越需要去注意的通道了。

可參考SENet模型結構。

（3） 混合域

了解前兩種注意力域的設計思路後，簡單對比一下。首先，空間域的注意力是忽略了通道域中的信息，將每個通道中的圖片特徵同等處理，這種做法會將空間域變換方法局限在原始圖片特徵提取階段，應用在神經網絡層其他層的可解釋性不強。

而通道域的注意力是對一個通道內的信息直接全局平均池化，而忽略每一個通道內的局部信息，這種做法其實也是比較暴力的行為。所以結合兩種思路，就可以設計出混合域的注意力機制模型

