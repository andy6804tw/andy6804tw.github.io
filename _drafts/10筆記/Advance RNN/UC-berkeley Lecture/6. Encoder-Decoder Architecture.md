## 前言
encoder-decoder 是一個非常熱門的模型架構，專門處理 Seq2seq 序列型的資料。再談談此架構前我們可以回顧 CNN 架構，我們也能把它當成是一種 encoder-decoder。CNN 模型經過一系列的訓練與 fine tune 後我們可以將 CNN 模型分為兩部分。在捲積層部分我們又稱特徵萃取(feature extractor)，輸入一張圖片並將它轉換成一組向量表示。接著我們將這組萃取後的特徵向量餵入全連階層(Dense layer)，在這一層網路負責做分類以及輸出類別。我們可以用 encoder-decoder 來解釋剛剛所提及的事情。在 Encoder 中輸入原始數據(raw input data)，並萃取出有用的特徵(intermediate presentation)讓神經網路可以快速抓到有用的資訊。然而 Decoder 是扮演分類器的角色，輸入 Encoder 所萃取的重要特徵並轉換成最終的分類輸出。

![](https://i.imgur.com/KPgFJ0B.png)

相同的在 RNN 模型中我們也能看成一個 encoder-decoder 架構。假設在語意分析例子中我們輸入一串句子。首先會將每個文字進行 Embedding 編碼然後再丟入 RNN，最終 Encoder 讀取所有文字訊息後得到一組輸出向量。此時 Encoder 輸出的這組向量足以表示輸入的那一串資訊。然而將 Encoder 輸出當作 Decoder 的輸入，Decoder 作為一個分類器角色，輸出結果是一個正向或負面的二元分類問題。

![](https://i.imgur.com/utSurVj.png)

## The Encoder-decoder Architecture
上述所提到的 CNN 和 RNN 都能解釋成 encoder-decoder 架構，並且得到不錯的結果。也就是說 encoder-decoder 架構在任何模型中我們可以分為兩部分，分別為 Encoder 和 Decoder。Encoder 接受一組輸入並將這些資料轉換成一組特徵向量我們稱作狀態(state)。假如是 LSTM 他的最終的 memory cell 輸出可以當作下圖中間的狀態(state)。Decoder 也可以是一個 RNN 的架構，它的 init state 為 Encoder 所提供接收一些輸入的資訊，除了這之外 Encoder 也有自己的輸入。其中 Decoder 的第一個輸入為 BOS 起始字元符號，代表句子的開始。在訓練過程中之後的 Decoder 輸入都有兩個一個是上一次的 hidden state 以及上一個的目標輸出。

![](https://i.imgur.com/YwrqxOz.png)

## Reference
- [簡報](https://courses.d2l.ai/berkeley-stat-157/slides/4_23/23-Encoder-decoder.pdf)
- [影片](https://www.youtube.com/watch?v=z9mvfjwSE38)