## 前言
接下來我們以一個實際的範例來解釋 encoder-decoder 架構。所謂的 Seq2seq 就是輸入一個序列，輸出另外一個序列。最典型的例子就是機器翻譯，輸入英文句子，輸出一個中文翻譯後句子。機器翻譯的困難點是輸入的序列長度不一定等於輸出的長度，一段很長的英文句子可能只要簡單的三、五個中文字就能清楚表示。因此在 Decoder 中何時要終止輸出是一個值得關注的問題，本文將會以一個 RNN 為底的模型來完整套用在 encoder-decoder 架構上。

![](https://i.imgur.com/jBhPswi.png)

## Seq2seq with RNN model
目前常見的 Seq2seq 模型分為兩個部分，分別有 Encoder 與 Decoder。其中 Encoder 負責接收一個序列輸入，這裡可以被視為是一個典型的 RNN 模型。首先 RNN 模型讀取一長串輸入的句子，透過時間序列逐一進入到 RNN 模型裡。RNN 的最後一個時間點輸出的 hidden state 會傳給 Decoder，因為通常最後一個時間點的 hidden state 已經看過了所有輸入資料，可能涵蓋了所有的輸入資訊。接著在 Decoder 中我們再使用另一個 RNN，它的初始狀態為 Encoder 最後一個時間點輸出的 hidden state。在 Decoder 的預測中第一個會先輸入 BOS 起始字元，接著會預測第一個文字。然而第一個時間點的輸出會再被送到下一個時間點的輸入，接著預測產生第二個時間點輸出。Decoder 輸出的方式依此類推，每個時間點的輸入為上一個時間點的輸出，直到設定的最大輸出長度或是終止符號 EOS 出現即停止。

![](https://i.imgur.com/3UezqZS.png)

## Encoder/Decoder Details
首先來源資料先進行 Embedding 編碼，得到一串固定長度的序列後依序進入 Encoder RNN 層。其中在 Encoder 最後一個時間點的每一層 RNN 的 hidden state 將會傳給 Decoder 作為 init state。這也意味著 Decoder 中的 RNN 層數需要與 Encoder RNN 層數相同，以及每一層的神經元個數也要相同才能共用 hidden state。另外在 Decoder 中多了一個全連接層(Dense Layer) 可以將 RNN 的輸出轉換成目標預測的任務。

![](https://i.imgur.com/YBSMyYM.png)

## Training
Decoder 在訓練和推論時有稍微不一樣，差別在於 Decoder 在每個時間點的輸入。在推論預測的階段每次 Decoder 的輸入為上一次 Decoder 的輸出。至於在訓練的時候每次 Decoder 輸入的內容為上一個時間點的 Ground True。

![](https://i.imgur.com/L5rqOej.png)


## 小結
以上為典型的 encoder-decoder 架構並採 RNN 模型來處理機器翻譯的問題。