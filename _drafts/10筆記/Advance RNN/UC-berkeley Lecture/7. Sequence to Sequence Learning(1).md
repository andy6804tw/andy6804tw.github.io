## 前言
接下來我們以一個實際的範例來解釋 encoder-decoder 架構。所謂的 Seq2seq 就是輸入一個序列，輸出另外一個序列。最典型的例子就是機器翻譯，輸入英文句子，輸出一個中文翻譯後句子。機器翻譯的困難點是輸入的序列長度不一定等於輸出的長度，一段很長的英文句子可能只要簡單的三、五個中文字就能清楚表示。因此在 Decoder 中何時要終止輸出是一個值得關注的問題，本文將會以一個 RNN 為底的模型來完整套用在 encoder-decoder 架構上。

![](https://i.imgur.com/jBhPswi.png)

## Seq2seq with RNN model
目前常見的 Seq2seq 模型分為兩個部分，分別有 Encoder 與 Decoder。其中 Encoder 負責接收一個序列輸入，這裡可以被視為是一個典型的 RNN 模型。首先 RNN 模型讀取一長串輸入的句子，透過時間序列逐一進入到 RNN 模型裡。RNN 的最後一個時間點輸出的 hidden state 會傳給 Decoder，因為通常最後一個時間點的 hidden state 已經看過了所有輸入資料，可能涵蓋了所有的輸入資訊。接著在 Decoder 中我們再使用另一個 RNN，它的初始狀態為 Encoder 最後一個時間點輸出的 hidden state。在 Decoder 的預測中第一個會先輸入 BOS 起始字元，接著會預測第一個文字。然而第一個時間點的輸出會再被送到下一個時間點的輸入，接著預測產生第二個時間點輸出。Decoder 輸出的方式依此類推，每個時間點的輸入為上一個時間點的輸出，直到設定的最大輸出長度或是終止符號 EOS 出現即停止。

![](https://i.imgur.com/3UezqZS.png)