# Linear Algebra
## 前言
學習深度學習之前要先了解矩陣的運算方式，首先我們先來大約複習一下大學所教過的線性代數。需多的深度學習架構不外乎都需要大量的運算，然而矩陣運算是一個非常有效率的計算方式。

## Scalars
在數學中，純量(scalar)是指用來定義向量空間的域的一個元素，只有大小沒有方向的，如數量、距離、速度或溫度。兩個純量之間可以做簡單的運算，例如彼此做相加以及相乘或是計算一個sin、cos函數。

![](https://i.imgur.com/3bbly7O.png)

## Vector
由多個純量描述的概念，比如方向、大小等都被稱為向量。然而一個向量也能做加法、乘法以及函式的運算。向量 a 的大小就是它的長度，一般用 |a|（在向量符號外加絕對值符號）來表示。另外歐幾里德範數(Euclidean norm)根據畢氏定理，它給出了從原點到點 x 之間的距離。

![](https://i.imgur.com/jc0N0JN.png)

由下圖可以看到藍色向量a加上黃色向量b可以得到綠色向量c。相同的向量縮放a重複b次得到c。在數學上他可以被平行的同時去計算，正是為何我們可以使用程式去實作並使用向量表示且透過 GPU 加速平行運算。

![](https://i.imgur.com/03ubZ9F.png)

### Dot product
點積 (Dot product) 又稱純量積。是一種接受兩個等長的數字序列（通常是坐標向量）、返回單個數字的代數運算。

```
兩個三維向量 [1, 3, 5] 和 [4, -2, 1] 的點積是
[1, 3, 5] · [4, -2, 1] = (1*4)+(3*2)+(5*1) = 3
```

正交是線性代數的概念，是垂直這一直觀概念的推廣。只有在一個確定的內積空間中才有意義。若內積空間中兩向量的內積為0，則稱它們是正交的。如果能夠定義向量間的夾角，則正交可以直觀的理解為垂直。

![](https://i.imgur.com/rtlRV4q.png)

點積還可以寫為：
![](https://i.imgur.com/DMHfoq4.png)
這裡，bt 是行向量 b 的轉置。使用上面的例子，一個1×3矩陣(行向量)乘以一個3×1矩陣(列向量)的行列式就是結果(通過矩陣乘法得到1×1矩陣):

![](https://i.imgur.com/h213OjN.png)
## Matrices
多個相同長度的一維向量合併在一起就稱為矩陣。直行橫列: 矩陣的每一橫排叫做一列(row)，每一直排叫做一行(column)。矩陣的運算方式跟先前提到的向量其實是一樣的。

![](https://i.imgur.com/v0NO2vZ.png)


![](https://i.imgur.com/xKzrUYd.png)

這裡要來複習矩陣相乘。在開始之前首先要了解行向量與列向量 (Row and column vectors)。透過矩陣乘法內積(inner product)可以將一個矩陣乘以一個向量得到另一個向量。

![](https://i.imgur.com/WJoJL1v.png)

兩個矩陣間也能進行乘法運算。

![](https://i.imgur.com/46L4cfP.png)

## Reference
- [簡報](https://courses.d2l.ai/berkeley-stat-157/slides/1_22/1-Logistics.pdf)
- [影片](https://courses.d2l.ai/berkeley-stat-157/units/introduction.html)