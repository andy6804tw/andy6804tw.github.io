
## 前言
深度學習有三大典型架構，分別為 DNN、CNN 與 RNN。除此之外 Attention 是這幾年熱門地架構，也許他還尚未開啟一股熱潮，但是我想 Attention 的潛力深不可測。它也可以算是在深度學習中的另一個大領域，在自然語言處理以及電腦視覺上。

## Attetntion Motivation
我們來看一下機器翻譯 Seq2seq 的架構，當我們 Decoding 不同字母時，Decoder 會參照 Encoder 中的每個字母相對位置進行翻譯。在某些情況下輸入的順序跟輸出所要翻譯的順序可能因為不同語言的文法會略有不同，因此 Decoder 在翻譯時會去關注 Encoder 中有興趣的部分。這裡再來回顧一下最早的 Seq2seq 模型在做什麼。Encoder 使用 RNN 將所有輸入資訊濃縮在一起成為一個 Hidden state。接著 Decoder 試著從 Encoder 輸出的最後一個 Hidden state 中提取重要資訊，這種最簡單的方法無法明確表達輸入的位置資訊(一大串輸入擠成一坨變成一個張量)。也許在 Decoder 中神經網路有默默地將位置順序進行考量，但是我們無法很明確地知道推論過程中是否有參照輸入位置資訊。

![](https://i.imgur.com/IjHqBte.png)

因此 Attetntion 的機制可以讓 Encoder 要關注的位置更加明確，在輸出的每一個文字都能透過 Attetntion 關注輸入中感興趣的位置。 Attetntion 有個特別的記憶的功能，它能將所有輸入以一對 key 和 value 做表示。下圖範例有三個輸入因此會有三對 key 和 value 代表每個輸入，這裡可以解釋為 memory input。在 Encoder 中的輸入稱為 query，我們給定一個 query 接著希望找到一組 key 與這個 query 越相近越好，代表目前的輸出要更加專注在輸入的哪一個片段。計算完成後就輸出，此時第一個輸出就完成了。

## Reference
- [簡報](https://courses.d2l.ai/berkeley-stat-157/slides/4_25/24-Attention.pdf)
- [影片](https://www.youtube.com/watch?v=SYIdimxpj6M&list=PLZSO_6-bSqHQHBCoGaObUljoXAyyqhpFW&index=126)