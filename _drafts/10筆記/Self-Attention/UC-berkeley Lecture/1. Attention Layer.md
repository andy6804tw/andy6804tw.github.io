
## 前言
深度學習有三大典型架構，分別為 DNN、CNN 與 RNN。除此之外 Attention 是這幾年熱門地架構，也許他還尚未開啟一股熱潮，但是我想 Attention 的潛力深不可測。它也可以算是在深度學習中的另一個大領域，在自然語言處理以及電腦視覺上。

## Attetntion Motivation
我們來看一下機器翻譯 Seq2seq 的架構，當我們 Decoding 不同字母時，Decoder 會參照 Encoder 中的每個字母相對位置進行翻譯。在某些情況下輸入的順序跟輸出所要翻譯的順序可能因為不同語言的文法會略有不同，因此 Decoder 在翻譯時會去關注 Encoder 中有興趣的部分。這裡再來回顧一下最早的 Seq2seq 模型在做什麼。Encoder 使用 RNN 將所有輸入資訊濃縮在一起成為一個 Hidden state。接著 Decoder 試著從 Encoder 輸出的最後一個 Hidden state 中提取重要資訊，這種最簡單的方法無法明確表達輸入的位置資訊(一大串輸入擠成一坨變成一個張量)。也許在 Decoder 中神經網路有默默地將位置順序進行考量，但是我們無法很明確地知道推論過程中是否有參照輸入位置資訊。

![](https://pic1.zhimg.com/80/v2-8ddc61785356dde25346f792914e920c_1440w.jpg)
![](https://i.imgur.com/IjHqBte.png)

因此 Attetntion 的機制可以讓 Encoder 要關注的位置更加明確，在輸出的每一個文字都能透過 Attetntion 關注輸入中感興趣的位置。 Attetntion 有個特別的記憶的功能，它能將所有輸入以一對 key 和 value 做表示。下圖範例有三個輸入因此會有三對 key 和 value 代表每個輸入，這裡可以解釋為 memory input。在 Encoder 中的輸入稱為 query，我們給定一個 query 接著希望找到一組 key 與這個 query 越相近越好，代表目前的輸出要更加專注在輸入的哪一個片段。計算完成後就輸出，此時第一個輸出就完成了。 Attention 就很像個資料庫，我們給一個 query 從這個 memory 中找到一個 key 然後計算回傳輸出。為了要使網路能夠進行學習，我們可以 query 每一個 key 時通過 soft-max 就能得到每一個需要專注多少的機率值，越大代表我們更需要關注。此時有適當的 loss 就能夠學習了。

![](https://i.ibb.co/NWDJh1B/Screen-Shot-2021-06-23-at-7-51-51-PM.png)

接著我們來看一下數學式，假設我們有 dq(query) 向量，和有 n 組 memory 每一個都有 key 與 value (k1, v1),...,(kn, vn)。首先我們要拿第一個 query 並得到 a1~an 分數，每個分數的計算方式是 query 與每一個 key 的相似度計算( score function)。之後得到這些分數我們在經過一個 soft-max 轉換可以得到 query 對於每一個 key 的機率值。也就是說目前的 query 輸出應該與原先輸入的內容，要特別關注地方，機率越大代表越重要。最終的輸出是一個 weighted sum 的數值，根據每一個計算的機率值。接下來我們會介紹典型兩種 Attenntion α 的計算方式，之後再進入到 Transformer 會再介紹另一種。

- score function
    - Dot Product Attention

![](https://i.ibb.co/f4gBWG2/Screen-Shot-2021-06-23-at-8-43-02-PM.png)

## Dot Product Attention
第一種計算 Attention score 的方法為 Dot Product Attention，以下的式子假設輸入(ki)和輸出(q)有相同維度的向量。similarity score 的計算就是 q 和 k 兩個向量的 inner product。假如你再除以一個 norm(|q||k|) 就等於向量 q、k 兩向量間的夾角。由於除以一個 norm 會導致難以計算梯度，以及 q 和 k 兩個向量的 inner product 每個元素各自做點積結果會變很大。


> 【為什麼除以dk?】 假設兩個dk 維向量每個分量都是一個相互獨立的服從標準正態分佈的隨機變量，那麼他們的點乘結果會變得很大，並且服從均值為0，方差就是dk，很大的點乘會讓softmax函數處在梯度很小的區域，對每一個分量除以sqrt(d_k) 可以讓點乘的方差變成1。

## 小節
這裡統整一下，首先我們給定一個 query，接著透過 key 去計算所有輸入的相似度並使用 soft-max 計算分數。最後輸出的值即為相關聯值。


## Reference
- [簡報](https://courses.d2l.ai/berkeley-stat-157/slides/4_25/24-Attention.pdf)
- [影片](https://www.youtube.com/watch?v=SYIdimxpj6M&list=PLZSO_6-bSqHQHBCoGaObUljoXAyyqhpFW&index=126)