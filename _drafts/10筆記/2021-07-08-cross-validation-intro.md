---
layout: post
title: '[機器學習] 交叉驗證 Cross-Validation 簡介'
categories: 'AI'
description:
keywords: Cross-Validation
---

## 前言
在解釋交叉驗證之前我們先來討論一下將資料集切分為訓練集、測試集和驗證集的問題。在一般狀況下我們會將資料先切割成兩等份，分別為訓練集和測試集。其中在模型訓練時，模型只會對訓練集進行擬和。另外測試集的資料並未參與訓練，因此可以拿來當作最終評估模型的好壞。我們的目標是要讓模型在訓練集和測試集都有不錯的成績，也就是說 loss 要越低越好。為了避免模型訓練發生過度擬和，通常我們還會從訓練集切一小部分資料出來進行驗證。驗證集的用處則是用來檢視模型在訓練過程中每次的迭代結果訓練的好不好。但該如何切出這個驗證集比較有公信力呢？如果我們僅切一小份的資料他是能有有效的評估訓練時模型的好壞嗎？在某些情況底下單純直接從資料集裡面切一塊出來當驗證集，是沒有辦法很有效的去評估一個模型訓練的好壞。說不定訓練出來的模型在這一份驗證集恰好表現得不錯，如果又隨機抽另一份資料來當驗證集說不定結果會變得很糟糕。這就表示模型泛化能力不足。為了避免這種情況發生並且有效的切割驗證集來評估模型，我們可以採用交叉驗證 Cross-Validation 的技巧來獲得最佳驗證。


## 什麼是交叉驗證?
簡單來說是將訓練資料進行分組，一部分做為訓練子集來訓練模型，另一部分做為驗證子集來評估模型。用訓練子集的數據先訓練模型，然後用驗證子集去跑一遍，看驗證集的損失函數(loss)或是分類準確率等。等模型訓練好之後，再用測試集去測試模型的性能。主要的交叉驗證法有下面這四個方法:

- holdout cross validation
- k-fold cross validation
- Leave one out cross validation
- Bootstrap

![](/images/posts/AI/2021/img1100708-1.png)

## Holdout Method
此方法是最經典且最簡單實作的交叉驗證法，Holdout 顧名思義就是將資料切出一部分作為模型評估的依據。在這種方法中，我們將資料隨機分為三部分：訓練集、驗證集和測試集。其中只有訓練集資料實際參與訓練，其餘的資料僅拿來評估模型好壞。驗證集使用時機是在訓練過程中可以檢視訓練的趨勢，若有發現過擬和跡象可以提早發現並解決。以及方便我們進行調整超參數以及選擇最佳的模型。當然僅透過驗證集不能代表全部，因此最後確定好模型時。我們會再拿事先切好的測試集進行最終的評估，檢視模型的泛化能力。

![](/images/posts/AI/2021/img1100708-2.png)

[參考](https://www.datavedas.com/holdout-cross-validation/)

## k-fold cross validation
上一個方法雖然簡單，但是在訓練過程中僅切一份驗證集往往不能夠代表全部。因此我們可以透過一些技巧切割驗證集，使得訓練過程中有一個更公正的評估方式。

## 小結
交叉驗證是訓練模型中非常重要的技巧，尤其是當手邊的資料集有限時更應該使用。透過交叉驗證技巧，即使在數據有限的情況下，我們也能夠獲得準確的結果，並且可以避免模型過度擬合。並為我們提供更準確的模型預測性能估計方式，同時也能夠提升模型的泛化能力。以上的方法可以直接使用 scikit-learn 裡面 model_selection 底下的 cross_val_score 方法進行實作。