# AI 小百科

## 端到端學習
依據人工智慧的進展我們賦予機器智慧的方法大致分成四種方法，下圖淺色部分人參與而深色部分是機器自動完成。效果上來說橫軸表示效果越來越好，複雜度上看，縱軸表示模型越來越複雜。下面我們分析一下端到端的優劣之處：

- 端對端的好處：通過縮減人工預處理和後續處理，儘可能使模型從原始輸入到最終輸出，給模型更多可以根據資料自動調節的空間，增加模型的整體契合度。
- 端對端的壞處：通過大量模型的組合，增加了模型複雜度，降低了模型可解釋性。

![](https://i.imgur.com/piWfV9C.png)

[來源](https://www.gushiciku.cn/dc_tw/109328837)

## 歸納偏置（inductive biases）
是從一些例子中尋找共性、泛化，形成一個比較通用的規則的過程，使得模型產生偏好。和貝葉斯學習中的先驗(Prior)有異曲同工之妙。從現實生活中觀察到的現像中歸納出一定的規則，選擇出更符合現實規則的模型。在卷積神經網絡中，我們假設特徵具有局部性(Locality)的特性，即當我們把相鄰的一些特徵放在一起，會更容易得到解。

## 消融實驗 (ablation study)
一些新穎的深度學習模型在論文中都會進行 Ablation Study，這部分的主要意義在於系統性的移除模型中的各種組件/trick等因子或者是創新的方法，來探究各個因素對於模型整體貢獻的強弱多寡，找到對性能最主要的影響因素。

## 統計學習 (Statistical learning)
統計學習可以說是機器學習的數理基礎，是以統計學及數學的觀點闡述(推導)機器學習演算法的理論、參數性質及模型評估。

## 預測 (prediction)
所謂的預測主要是希望能夠估計一個函數 `f()` 使得輸入自變數 (x) 得到一個預測輸出 (y^)。目標是模型預測的 y^ 要與真實的答案 y 越接近越好，也就是誤差越小越好。

## 推論 (inference)
推論的目的也是要估計一個好的函數 `f()`。不過推論所關注的內容在於觀察輸入特徵 (x) 與輸出 (y) 之間的關聯性。例如觀察房間坪數是否與房價呈現線性關聯。

## 機器學習中經常所說的魯棒性
在機器學習領域，總是看到`演算法的魯棒性`這類字眼，比如這句--L1函數比L2函數魯棒。`魯棒`的英文是robustness。

 Huber從穩健統計的角度系統地給出了魯棒性3個層面的概念：
 - 模型具有較高的精度或有效性，這也是對於機器學習中所有學習模型的基本要求。
 - 對於模型假設出現的較小偏差，只能對演算法效能產生較小的影響；主要是：噪聲。
 - 對於模型假設出現的較大偏差，不可對演算法效能產生災難性的影響。主要是：離群點。

結論是越 robust 的模型不會因為噪聲和離群點受影響。常常訓練樣本和預測樣本的分佈不大相同，一個具有魯棒性的模型就是即使當測試集的數據分佈與訓練集數據分佈比較不同的時候，模型也能給出較好的預測結果。

在機器學習，訓練模型時，工程師可能會向算法內添加噪聲（如對抗訓練），以便測試算法的「魯棒性」。可以將此處的魯棒性理解述算法對數據變化的容忍度有多高。魯棒性並不同於穩定性，穩定性通常意味着「特性隨時間不變化的能力」，魯棒性則常被用來描述可以面對複雜適應系統的能力，需要更全面的對系統進行考慮。就好比模型不光是夏天好，冬天也要好。


## MAE
使用 MAE 最小化時會產生偏差，因為他會瞄準中位數對比較多的那群依靠。
對於存在異常值的情況，中位數比平均數的魯棒性更強。在供應鏈產業中，這一點尤為重要，因為我們要面對很多異常點。

例如:
若數據中90%的樣本對應的目標值為150，剩下10%在0到30之間。那麼使用MAE作為損失函數的模型可能會忽視10%的異常點，而對所有樣本的預測值都為150。這是因為模型會按中位數來預測。而使用MSE的模型則會給出很多介於0到30的預測值，因為模型會向異常點偏移。

## MSE
RMSE 會為大的誤差值分配高權重，同時也要付出代價，就是對異常點過於敏感。


### 小總結
如果我們最小化 MSE 來對所有的樣本點只給出一個預測值，那麼這個值一定是所有目標值的平均值。但如果是最小化 MAE，那麼這個值，則會是所有樣本點目標值的中位數。對異常值而言，中位數比均值更加魯棒，因此 MAE 對於異常值也比 MSE 更穩定。

- MAE 會忽略異常值，而 RMSE 會注意到異常值並得到沒有偏差的預測。
- 如果使用 MAE 作為性能評估指標會得到很大偏差，你可能需要使用 MSE。
- 如果數據集包含很多異常值，導致預測結果產生偏移，你可能需要用 MAE。


# 影像視覺
## 平移不變性（translation equivariance）
在影像分類任務中，平移不變性就是影像中的目標不管被移動到哪個位置，模型給出的標籤應該都是相同的。

## Feature Pyramid Network
那就是跳連線相加可以實現不同解析度特徵的組合，因為淺層容易有高解析度但是低階語義的特徵，而深層的特徵有高階語義，但解析度就很低了。(DenseNet)
